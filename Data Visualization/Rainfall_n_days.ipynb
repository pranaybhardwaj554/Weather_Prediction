{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda65ecf",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f35730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aea18",
   "metadata": {},
   "source": [
    "# Importing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5b238",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "files = [\"Delhi\",\n",
    "         \"East_20\", \"East_100\", \"East_500\",\n",
    "         \"North_20\", \"North_100\",\"North_500\",\n",
    "         \"South_20\", \"South_100\",\"South_500\",\n",
    "         \"West_20\", \"West_100\", \"West_500\"]\n",
    "#List containing the data for different cities\n",
    "data = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    data_ = pd.read_csv(\"data/\" + files[i] + \".csv\")\n",
    "\n",
    "    if i != 0:\n",
    "        data_.pop(\"date\")\n",
    "    else:\n",
    "        rs = np.random.RandomState(0)\n",
    "        corr = data_.corr()\n",
    "        sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "                    annot=True)\n",
    "    data.append(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2086e8",
   "metadata": {},
   "source": [
    "# Preprocessor class that preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50605302",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "#days denote size of the window\n",
    "    def __init__(self, data, attr, cutoff,  days = 1):\n",
    "        # X and Y data\n",
    "        self.X = np.zeros((data[0].shape[0] - days, ((data[0].shape[1] - 1)*len(data))*days))\n",
    "        self.Y = np.zeros((self.X.shape[0], 1))\n",
    "        \n",
    "        #Concatination of cities data;\n",
    "        data_cities = data[0]\n",
    "        for i_ in range(1, len(data)):\n",
    "            data_cities = pd.concat([data_cities, data[i_]], axis = 1)\n",
    "        \n",
    "        data_cities = data_cities.to_numpy()\n",
    "        date_ = data[0][\"date\"]\n",
    "        date = [[], [], []]\n",
    "        \n",
    "        #Data filing\n",
    "        for i_ in range(0, self.X.shape[0]):\n",
    "            for offset in range(0, days):\n",
    "                index = (offset * (data_cities.shape[1] - 1))\n",
    "                self.X[i_, index: index + data_cities.shape[1] - 1] = data_cities[i_ + offset, 1:]\n",
    "            self.Y[i_, 0] = data[0][attr][i_+days]\n",
    "            day = int(date_[i_][ : date_[i_].index(\"-\")])\n",
    "            month = int(date_[i_][date_[i_].index(\"-\") + 1 : date_[i_].rindex(\"-\")])\n",
    "            year = int(date_[i_][date_[i_].rindex(\"-\") + 1 : ])\n",
    "            date[0].append(day); date[1].append(month); date[2].append(year)\n",
    "        \n",
    "        self.X = np.append(self.X, np.array(date).T, axis = 1)\n",
    "        self.Y[self.Y <= cutoff] = 0\n",
    "        self.Y[self.Y > cutoff] = 1\n",
    "        \n",
    "        self.standardize_data()\n",
    "    \n",
    "    #Standardizing the data\n",
    "    def standardize_data(self):\n",
    "        mean = np.mean(self.X, axis = 0)\n",
    "        std = np.std(self.X, axis = 0)\n",
    "        std[std == 0] = 1\n",
    "        self.X = self.X - mean\n",
    "        self.X = self.X / std\n",
    "        \n",
    "    #Splitting the data\n",
    "    #def split(self):\n",
    "\n",
    "def coeff_printer(file_order, given_model, data, days, consider_date=False):\n",
    "    index = 0\n",
    "    if consider_date:\n",
    "        print(\"year:\", given_model.coef_[index])\n",
    "        index += 1\n",
    "        print(\"year_day:\", given_model.coef_[index])\n",
    "        index += 1\n",
    "\n",
    "    for day in range(days, 0, -1):\n",
    "        for file in file_order:\n",
    "            for (attr, val) in data[0].iteritems():\n",
    "                if attr != \"date\":\n",
    "                    str_coef = f\"{round(given_model.coef_[index], 4)}\"\n",
    "                    print(f\"Before: {str(day):4s} | file: {file:10s} | attr: {attr:40s}\",\n",
    "                          f\":{str_coef:>10s}\")\n",
    "                    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3006b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Here we have changed the value of days and then train different models for that data \n",
    "obj = PreProcessor(data, \"Total Precipitation (MM)\", 2.5, days = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a0116",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04297cab",
   "metadata": {},
   "source": [
    "# Splitting up the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f029c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(obj.X, obj.Y, test_size=0.20, random_state=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d048773",
   "metadata": {},
   "source": [
    "## Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08350b8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(max_iter = 100000).fit(x_train, np.ravel(y_train))\n",
    "y_hat = clf.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6daac8",
   "metadata": {},
   "source": [
    "## Defining score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ca166",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,precision_score,accuracy_score,plot_roc_curve\n",
    "def score(y, y_hat):\n",
    "    print(\"Recall is\",recall_score(y, y_hat))\n",
    "    print(\"Accuracy is\",accuracy_score(y, y_hat))\n",
    "    print(\"Precision is\",precision_score(y, y_hat))\n",
    "\n",
    "def give_y(y, threshold):\n",
    "    y_hat = np.copy(y[:,1])\n",
    "    y_hat[y_hat >= threshold] = 1\n",
    "    y_hat[y_hat < threshold] = 0\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de076e8",
   "metadata": {},
   "source": [
    "## Roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74641d0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "metrics.plot_roc_curve(clf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f139bea",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Threshold set at 0.5\n",
    "y_hat1 = clf.predict(x_test)\n",
    "score(y_test, y_hat1)\n",
    "y_hat2 = clf.predict(x_train)\n",
    "score(y_train, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7f26",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a09e4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state = 0, max_depth = 13)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "y_hat1 = model.predict(x_train)\n",
    "score(y_train, y_hat1)\n",
    "y_hat2 = model.predict(x_test)\n",
    "score(y_test, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89570d63",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fadeb",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), n_estimators=100, random_state=0)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "y_hat1 = model.predict(x_train)\n",
    "score(y_train, y_hat1)\n",
    "y_hat2 = model.predict(x_test)\n",
    "score(y_test, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae5f65",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf559a47",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(alpha = 0.001,random_state=1,validation_fraction=0.1 ,hidden_layer_sizes=(128, 64, 32, 16), max_iter=10000).fit(x_train, np.ravel(y_train))\n",
    "y_hat1 = model.predict(x_train)\n",
    "y_hat2 = model.predict(x_test)\n",
    "score(y_train, y_hat1)\n",
    "score(y_test, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745db5e1",
   "metadata": {},
   "source": [
    "## Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c304bcc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "y_hat1 = model.predict(x_train)\n",
    "y_hat2 = model.predict(x_test)\n",
    "score(y_train, y_hat1)\n",
    "score(y_test, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057b425",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae48fb6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=9)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "y_hat1 = model.predict(x_train)\n",
    "y_hat2 = model.predict(x_test)\n",
    "score(y_train, y_hat1)\n",
    "score(y_test, y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e9bac",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "accuracy_train = []\n",
    "precision_train = []\n",
    "recall_train = []\n",
    "accuracy_test = []\n",
    "precision_test = []\n",
    "recall_test = []\n",
    "for i in range(2,13):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(x_train, np.ravel(y_train))\n",
    "    y_hat = model.predict(x_test)\n",
    "    accuracy_test.append(accuracy_score(y_test, y_hat))\n",
    "    recall_test.append(recall_score(y_test, y_hat))\n",
    "    precision_test.append(precision_score(y_test, y_hat))\n",
    "    y_hat1 = model.predict(x_train)\n",
    "    accuracy_train.append(accuracy_score(y_train, y_hat1))\n",
    "    recall_train.append(recall_score(y_train, y_hat1))\n",
    "    precision_train.append(precision_score(y_train, y_hat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76eab24",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(range(2,13)), accuracy_train)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.rc('grid', linestyle=\":\", color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e22d86",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}